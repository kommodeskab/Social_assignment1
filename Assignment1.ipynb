{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution:\n",
    "| Parts | Name  | Student id |\n",
    "|:---------|:--------:|:--------:|\n",
    "|  Part 1   |  Row 1   | Row 1 |\n",
    "|  Part 2   |  Row 2   | Row 2 |\n",
    "|  Part 3   |  Row 3   | Row 3 |\n",
    "|  Part 4   |  Row 3   | Row 3 |\n",
    "\n",
    "On the github it looks like it only one who contributed with the majority of the code, but that simply because we worked on it in the classes and On GitHub, it looks like only one contributed to most of the code, but that is simply because we worked on it in the classes and kept the final version on one PC. We have all contributed to the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andba\\AppData\\Local\\Temp\\ipykernel_3164\\1662815981.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Web-scraping\n",
    "## ANSWERS TO QUESTIONS:\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url2019 = \"https://ic2s2-2023.org/program\"\n",
    "\n",
    "page = requests.get(url2019)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "## find all classes \"nav_list\"\n",
    "nav_lists = soup.find_all(class_=\"nav_list\")\n",
    "\n",
    "names = []\n",
    "for nav_list in nav_lists:\n",
    "    new_names = nav_list.find_all(\"i\")\n",
    "    new_names = [name.get_text() + \",\" for name in new_names]\n",
    "    new_names = \" \".join(new_names)\n",
    "    new_names = new_names.split(\", \")\n",
    "    new_names = [name.strip() for name in new_names]\n",
    "    names.extend(new_names)\n",
    "    \n",
    "unique_names = list(set(names))\n",
    "unique_names_sorted = [name for name in unique_names if \" \" in name]\n",
    "np.save(\"unique_names.npy\", unique_names_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique names:  1499\n",
      "First 10 unique names:  ['Telmo Menezes' 'Lui Ruck' 'James Calum Young' 'Giyeon Baek'\n",
      " 'Karolina Stanczak' 'Mathieu Génois' 'Karthikeya Kaushik' 'James Evans'\n",
      " 'Giada Marino' 'Tom Emery']\n"
     ]
    }
   ],
   "source": [
    "unique_names = np.load(\"unique_names.npy\")\n",
    "print(\"Number of unique names: \", len(unique_names))\n",
    "print(\"First 10 unique names: \", unique_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "Explain the process you followed to web-scrape the page. Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices (answer in max 150 words).\n",
    "### Answer:\n",
    "In total, there are 1499 unique names of researchers. This is achieved by web-scraping of the page given in the assignment with the use of Beautifulsoup as a library for this. \n",
    "We looked through the page inspection to see where the names were placed. We found that they belonged to a class called “nav_list” in which the names were listed in the “i” place, which we collected. Some of the names were combined, so we had to ensure that we correctly separated the names by looking for “,”. The next thing was to find out if some were repeated, to do that we first turned the list into a set and then to a list. This is an effective way to ensure that there are no repeated names. In the end, we ensured that all the names contained a space, to ensure it was names.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Ready made vs Custom made data\n",
    "### Question:\n",
    "\n",
    "What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book (answer in max 150 words).\n",
    "### Answer:\n",
    "A positive aspect of the custom-made data used in Centola's experiment is that it is specifically designed to answer the research question. Another positive thing about this study that is not always true about custom-made data, is the fact that people did not know they were being observed, it is \"non-reactive\". Another possible upside is that it is likely quite \"clean\" data, as it is collected in a controlled environment. A downside can be the overhead, resources and time needed to collect the relatively small amount of data. The ready-made data has some of the classic positive aspects such as: The size, non-reactive and it is always on. One has to be careful making sure no sensitive information is shared with the public. Algorithmic confounding and drift inherent to longitudinal app data can be a downside.\n",
    "\n",
    "### Question:\n",
    "How do you think these differences can influence the interpretation of the results in each study? (answer in max 150 words)\n",
    "\n",
    "### Answer:\n",
    "One of the main differences between the interpretation of the results in Centola's experiment versus Nicolaides's study is the fact that it is more likely to observe and conclude causality in the custom-made data. This is because the experiment is designed to answer a specific question and the data is collected in a controlled environment. In the ready-made data, it is more likely to observe correlation, as the data is not collected to answer a specific research question and will be thus be incomplete. The ready-made data is also more likely to be affected by algorithmic confounding and drift, which can make it harder to interpret the results. The ready-made data is also more likely to be \"dirty\", full of noise and outliers, which can make it harder to interpret the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Gathering Research Articles using the OpenAlex API\n",
    "\n",
    "We haven't included all code for this section. This is due to the fact, that we ran the code both for the original authors and for the coatuhors, and we thought it would be redundant to have twice. We included the functions used to extract the information below.\n",
    "\n",
    "### Question:\n",
    "__How many works are listed in your IC2S2 papers dataframe? How many unique researchers have co-authored these works?__ <br>\n",
    "### Answer:\n",
    "In the end we found 10540 unique authors and 68428 papers. \n",
    "### Question:\n",
    "__Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time?__\n",
    "### Answer:\n",
    "Unfortunately, we couldn't get multiprocessing to work. To speed up the process slightly, we used filters: <br>\n",
    "<code> FILTERS = \",cited_by_count:>10,authors_count:<10\" </code> <br>\n",
    "<br>\n",
    "Also, we tried to quickly cancel our search if the paper didn't uphold the requirements. For example, if the work count wasnt between 5 and 5000, we immediatly ended the query. \n",
    "### Question:\n",
    "__Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices?__\n",
    "### Answer:\n",
    "One reason for making these threshols is to filter out outliers, i.e. papers that are either corrupted or anomalies. Also, since we're interested in computational social science, it makes sense to only look at papers that falls within this field. Filter away authors with too many works and papers with too make authors is also a way of limiting the size of our dataset so that it is actually manageable to work with. Obviously, we miss some results in the process, especially: the papers that are collaborations between many authors, and the many \"junk\" papers (they are obviously not junk, but) with 0 citations for example those by aspiring phd studets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script for finding the information about the authors\n",
    "We find:\n",
    "- id\n",
    "- display_name\n",
    "- works_api_url\n",
    "- h_index\n",
    "- works_count\n",
    "- country_code\n",
    "\n",
    "We use the OpenAlex API to find the information about the authors\n",
    "\"\"\"\n",
    "\n",
    "from Levenshtein import distance as lev\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "## The things we want to extract\n",
    "columns = [\"id\", \"display_name\", \"works_api_url\", \"h_index\", \"works_count\", \"country_code\"]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "## The base url for the API\n",
    "BASE_URL = \"https://api.openalex.org/authors?search=\"\n",
    "## The unique names we want to search for\n",
    "names = np.load(\"unique_names.npy\")\n",
    "\n",
    "def get_information_from_name(name):\n",
    "    \"\"\"Get information about an author from the OpenAlex API\"\"\"\n",
    "    url = BASE_URL + name.lower()\n",
    "    response = requests.get(url)\n",
    "    response_json = response.json()\n",
    "    results = response_json[\"results\"]\n",
    "\n",
    "    if results:\n",
    "        ## Look at display names but also alternative names\n",
    "        ## to see if we can find a match\n",
    "        display_name = results[0][\"display_name\"]\n",
    "        display_name_alternatives = results[0][\"display_name_alternatives\"]\n",
    "        display_name_alternatives.append(display_name)\n",
    "\n",
    "        ## Calculate the Levenshtein distance between the name we are looking for\n",
    "        ## Levenshtein distance is the number of single-character edits (insertions, deletions or substitutions)\n",
    "        ## required to change one word into the other\n",
    "        ## If the distance is less than 2, we consider it a match (this is a bit arbitrary, but we had to set a threshold)\n",
    "        lev_distances = [lev(name, alt) for alt in display_name_alternatives]\n",
    "\n",
    "        if min(lev_distances) < 2:\n",
    "            ## If we have a match, we extract the information we want\n",
    "            id = results[0][\"id\"]\n",
    "            works_api_url = results[0][\"works_api_url\"]\n",
    "            h_index = results[0][\"summary_stats\"][\"h_index\"]\n",
    "\n",
    "            count_by_year = results[0][\"counts_by_year\"]\n",
    "            works_count = count_by_year[0][\"works_count\"] if count_by_year else None\n",
    "\n",
    "            affiliations = results[0][\"affiliations\"]\n",
    "            country_code = affiliations[0][\"institution\"][\"country_code\"] if affiliations else None\n",
    "\n",
    "            new_row = pd.Series(data=[id, display_name, works_api_url, h_index, works_count, country_code], index=columns)\n",
    "            return new_row\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "## We loop through all the names and extract the information\n",
    "## We add the information to a dataframe\n",
    "for name in tqdm(names):\n",
    "    new_row = get_information_from_name(name)\n",
    "    if new_row is not None:\n",
    "        df.loc[len(df)] = new_row\n",
    "            \n",
    "df.to_pickle(\"authors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use the above information to find the works of the authors\n",
    "We find:\n",
    "- id\n",
    "- publication_year\n",
    "- cited_by_count\n",
    "- author_ids\n",
    "- title\n",
    "- abstract_inverted_index\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_pickle(\"authors.pkl\")\n",
    "works_api_urls = df[\"works_api_url\"].tolist()\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "paper_df = pd.DataFrame(columns = [\"id\", \"publication_year\", \"cited_by_count\", \"author_ids\"])\n",
    "abstracts_df = pd.DataFrame(columns = [\"id\", \"title\", \"abstract_inverted_index\"])\n",
    "\n",
    "COMSCOSCI = [\"Sociology\", \"Psychology\", \"Economics\", \"Political Science\"]\n",
    "QUANSCI = [\"Mathematics\", \"Physics\", \"Computer Science\"]\n",
    "\n",
    "FILTERS = \",cited_by_count:>10,authors_count:<10\"\n",
    "PARAMS = {\"per-page\": \"200\", \"page\": \"1\"}\n",
    "\n",
    "def get_works_from_url(works_api_urls, filters, params):\n",
    "    for attempt in range(10): ## for some reason, the requests sometimes fail, so we need to keep trying until it works\n",
    "                ## usually, it works on the first try, but sometimes it takes 2 or 3 tries\n",
    "        try:    ## use a try-except block to catch the error and keep going\n",
    "            url += FILTERS\n",
    "            \n",
    "            ## get the first page of results to find out how many pages there are\n",
    "            ## and how many works there are in total\n",
    "            response = requests.get(url, params=PARAMS).json()\n",
    "            work_counts = response[\"meta\"][\"count\"] \n",
    "            number_of_pages = work_counts // 200 + 1\n",
    "            \n",
    "            ## we only want to get works that have between 5 and 5000 citations\n",
    "            if 5 <= work_counts <= 5000:\n",
    "                ## loop through all the pages of results\n",
    "                for page_number in range(1, number_of_pages + 1):\n",
    "                    ## get the next page of results\n",
    "                    new_params = {\"per-page\": \"200\", \"page\": str(page_number)}\n",
    "                    response = requests.get(url, params=new_params).json()                    \n",
    "                    works = response[\"results\"]\n",
    "\n",
    "                    ## loop through all the works on the page\n",
    "                    for work in works:\n",
    "                        ## get the id of the work\n",
    "                        ## if the work is not already in the dataframe, add it\n",
    "                        id = work[\"id\"]\n",
    "                        \n",
    "                        if id not in paper_df[\"id\"].tolist():\n",
    "                            \n",
    "                            ## get the publication year, number of citations, title, and abstract\n",
    "                            publication_year = work[\"publication_year\"]\n",
    "                            cited_by_count = work[\"cited_by_count\"]\n",
    "                            title = work[\"title\"]\n",
    "                            abstract_inverted_index = work[\"abstract_inverted_index\"]\n",
    "                            \n",
    "                            authors_list = work[\"authorships\"]\n",
    "                            authors_ids = [author[\"author\"][\"id\"] for author in authors_list]\n",
    "                            \n",
    "                            concepts_list = work[\"concepts\"]\n",
    "                            concepts = [concept[\"display_name\"] for concept in concepts_list]\n",
    "                            \n",
    "                            ## check if the work is relevant\n",
    "                            ## ie if it is in both the COMSCOSCI and QUANSCI categories\n",
    "                            relevant = any(concept in COMSCOSCI for concept in concepts) and any(concept in QUANSCI for concept in concepts)\n",
    "                            \n",
    "                            if relevant:\n",
    "                                new_paper_row = pd.Series([id, publication_year, cited_by_count, authors_ids], index = paper_df.columns)\n",
    "                                paper_df.loc[len(paper_df)] = new_paper_row\n",
    "                                \n",
    "                                new_abstract_row = pd.Series([id, title, abstract_inverted_index], index = abstracts_df.columns)\n",
    "                                return new_abstract_row\n",
    "                  \n",
    "        ## if the request fails, print the error and try again\n",
    "        ## we will try 10 times before giving up              \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    ## if we have tried 10 times and it still doesn't work, return None\n",
    "    return None \n",
    "\n",
    "## loop through all the authors\n",
    "## for each author, get all the works they have written\n",
    "## and add the works to the dataframe\n",
    "for url in tqdm(works_api_urls):\n",
    "    new_abstract_row = get_works_from_url(url, FILTERS, PARAMS)\n",
    "    if new_abstract_row is not None:\n",
    "        abstracts_df.loc[len(abstracts_df)] = new_abstract_row\n",
    "        \n",
    "paper_df.to_pickle(\"papers.pkl\")\n",
    "abstracts_df.to_pickle(\"abstracts.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then repeat the same procedure for all the coauthors. <br>\n",
    "i.e. we find information about the new authors and find their work. <br>\n",
    "This amounts to finding all the authors that we haven't already looked at and running them through the same functions. <br>\n",
    "In the end, we end up with these dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of authors:  10540\n",
      "Final number of papers:  68428\n"
     ]
    }
   ],
   "source": [
    "final_authors = pd.read_pickle(\"final_authors.pkl\")\n",
    "final_papers = pd.read_pickle(\"final_papers.pkl\")\n",
    "\n",
    "print(\"Final number of authors: \", len(final_authors))\n",
    "print(\"Final number of papers: \", len(final_papers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: The Network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andba\\AppData\\Local\\Temp\\ipykernel_3164\\619202017.py:20: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  authors = authors.fillna(0)\n",
      "C:\\Users\\Andba\\AppData\\Local\\Temp\\ipykernel_3164\\619202017.py:28: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  authors = authors.fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "## load in the data again\n",
    "## we need to do this because we have to re-run the previous cells\n",
    "papers = pd.read_pickle(\"final_papers.pkl\")\n",
    "authors = pd.read_pickle(\"final_authors.pkl\")\n",
    "\n",
    "\"\"\"\n",
    "For each author, we want to find the total number of citations they have received\n",
    "and the earliest year they have published a paper\n",
    "We will add these columns to the authors dataframe\n",
    "\"\"\"\n",
    "\n",
    "## use the explode function to very quickly create a new row for each author of a paper\n",
    "citations = papers.explode(\"author_ids\").groupby(\"author_ids\")[\"cited_by_count\"].sum().reset_index()\n",
    "## add this column to the authors dataframe\n",
    "authors = authors.merge(citations, left_on=\"id\", right_on=\"author_ids\", how=\"left\")\n",
    "authors = authors.rename(columns={\"cited_by_count\": \"total_citations\"})\n",
    "authors = authors.drop(columns=\"author_ids\")\n",
    "authors = authors.fillna(0)\n",
    "\n",
    "earliest_publication = papers.explode(\"author_ids\").groupby(\"author_ids\")[\"publication_year\"].min().reset_index()\n",
    "## add this column to the authors dataframe\n",
    "## some authors have no publication year, so we will fill in a zero\n",
    "authors = authors.merge(earliest_publication, left_on=\"id\", right_on=\"author_ids\", how=\"left\")\n",
    "authors = authors.rename(columns={\"publication_year\": \"earliest_publication\"})\n",
    "authors = authors.drop(columns=\"author_ids\")\n",
    "authors = authors.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes:  9964\n",
      "Number of edges:  32742\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "\"\"\"\n",
    "We want to create a graph where the nodes are authors and the edges are the number of papers they have written together\n",
    "We will use the NetworkX library to create this graph\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Get all unique pairs of author_ids from the papers dataframe, one line at a time\n",
    "edgelist_dict = {}\n",
    "for index, row in papers.iterrows():\n",
    "    author_ids = row['author_ids']\n",
    "    citations = row['cited_by_count']\n",
    "    for pair in itertools.combinations(author_ids, 2):\n",
    "        pair = tuple(sorted(pair))\n",
    "        # If the pair is already in the dictionary, increment the count\n",
    "        if pair in edgelist_dict:\n",
    "            edgelist_dict[pair] += citations\n",
    "        # If the pair is not in the dictionary, add it\n",
    "        else:\n",
    "            edgelist_dict[pair] = citations\n",
    "\n",
    "edgelist = [(k[0], k[1], v) for k, v in edgelist_dict.items()]\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(edgelist)\n",
    "\n",
    "print(\"Number of nodes: \", G.number_of_nodes())\n",
    "print(\"Number of edges: \", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add the attributes of the authors to the graph\n",
    "We will add the display name, country code, total citations, and earliest publication year\n",
    "\"\"\"\n",
    "\n",
    "for row in authors.iterrows():\n",
    "    row = row[1]\n",
    "    \n",
    "    attributes_dict = {}\n",
    "    attributes_dict[\"display_name\"] = row[\"display_name\"]\n",
    "    attributes_dict[\"country_code\"] = row[\"country_code\"]\n",
    "    attributes_dict[\"total_citations\"] = row[\"total_citations\"]\n",
    "    attributes_dict[\"earliest_publication\"] = row[\"earliest_publication\"]\n",
    "    G.add_node(row[\"id\"], **attributes_dict)\n",
    "    \n",
    "## save network as json\n",
    "import json\n",
    "data = nx.node_link_data(G)\n",
    "with open('network.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network density: 0.00016581743345606923\n"
     ]
    }
   ],
   "source": [
    "num_edges = G.number_of_edges()\n",
    "num_nodes = G.number_of_nodes()\n",
    "\n",
    "## calculate density of the network\n",
    "density = nx.density(G)\n",
    "print(\"Network density:\", density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the network connected? False\n"
     ]
    }
   ],
   "source": [
    "## is the network fully connected?\n",
    "is_connected = nx.is_connected(G)\n",
    "print(\"Is the network connected?\", is_connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 10007\n"
     ]
    }
   ],
   "source": [
    "## how many clusters are there? \n",
    "clusters = list(nx.connected_components(G))\n",
    "print(\"Number of clusters:\", len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of isolated nodes: 9909\n"
     ]
    }
   ],
   "source": [
    "## how many isolated nodes are there?\n",
    "isolated_nodes = list(nx.isolates(G))\n",
    "print(\"Number of isolated nodes:\", len(isolated_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average degree: 3.295124037639008\n",
      "Median degree: 1\n",
      "Mode degree: 0\n",
      "Minimum degree: 0\n",
      "Maximum degree: 428\n",
      "Average strength: 505.9961757157953\n",
      "Median strength: 13\n",
      "Mode strength: 0\n",
      "Minimum strength: 0\n",
      "Maximum strength: 431675\n"
     ]
    }
   ],
   "source": [
    "## Compute the average, median, mode, minimum, and maximum degree of the nodes. Perform the same analysis for node strength (weighted degree).\n",
    "degrees = [val for (node, val) in G.degree()]\n",
    "strengths = [val for (node, val) in G.degree(weight=\"weight\")]\n",
    "\n",
    "print(\"Average degree:\", sum(degrees) / len(degrees))\n",
    "print(\"Median degree:\", sorted(degrees)[len(degrees) // 2])\n",
    "print(\"Mode degree:\", max(set(degrees), key=degrees.count))\n",
    "print(\"Minimum degree:\", min(degrees))\n",
    "print(\"Maximum degree:\", max(degrees))\n",
    "\n",
    "print(\"Average strength:\", sum(strengths) / len(strengths))\n",
    "print(\"Median strength:\", sorted(strengths)[len(strengths) // 2])\n",
    "print(\"Mode strength:\", max(set(strengths), key=strengths.count))\n",
    "print(\"Minimum strength:\", min(strengths))\n",
    "print(\"Maximum strength:\", max(strengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: The Network of Computational Social Scientists\n",
    "### Question:\n",
    "Discuss the results above on network density, and connectivity. Are your findings in line with what you expected? Why? (answer in max 150 words)\n",
    "\n",
    "## ANSWER TO QUESTION:\n",
    "We begin with the idea that the network is sparse, due to geographical distance and many different reachesfields. In total, we find 9964 nodes (researchers) and 32742 links (collaborations). This results in a density of 0.000166, which makes sense since the amount of links is small compared to the number of possible connections and makes the network very sparse. Furthermore, this means there have to be a lot of individuals. Looking at the amount of clusters and isolated nodes will provide insights into how the distribution of these groups and individuals. It turns out that there are 10007 clusters and 9909 isolated nodes. The amount of clusters means that some communities collaborated and the isolated nodes say that the majority of the scientists only interact in limited collaboration. This is likely because the researchers are from different fields and geographical locations. This is in line with what we expected.\n",
    "\n",
    "### Question:\n",
    "Compute the average, median, mode, minimum, and maximum degree of the nodes. Perform the same analysis for node strength (weighted degree). What do these metrics tell us about the network? (answer in max 150 words)\n",
    "\n",
    "## ANSWER TO QUESTION:\n",
    "An average degree of 3.30 tells us that on average, each researcher is connected to 3.30 other researchers. The median degree of 1 tells us that half of the researchers are connected to 1 or fewer researchers. The minimum is unsurprisingly 0 since there are isolated nodes. The maximum is 428, which is a very high number, this tells us that the network is not very connected, but there are some researchers who are very connected. The average strength, i.e. the weighted degree is 506, and the median strength is 13, which means that the network is skewed, with a few researchers having a very high strength, and most researchers having a very low strength. This is likely due to the fact that the network is very sparse and not very connected.  \n",
    "\n",
    "### Question:\n",
    "Identify the top 5 authors by degree. What role do these node play in the network?\n",
    "Research these authors online. What areas do they specialize in? Do you think that their work aligns with the themes of Computational Social Science? If not, what could be possible reasons? (answer in max 150 words)\n",
    "\n",
    "## ANSWER TO QUESTION:\n",
    "The top 5 authors by degree are:\n",
    "428, Chao Wang: Seems to primarily work with Chemistry and Materials Science, but has also published in the field of Applied Physics, which coudl be the reason we see him in the network. \n",
    "417, Hui Wang: Works with genetics and diseases mostly, but has publications in \"Science\".\n",
    "313, Hao Chen: Nanotechnology and organic materials. \n",
    "284, Qi Wang: Has studied both social interactions and things such as physiology and materials science.\n",
    "189, Xiao Zhang: Has published in journals such as \"Econometrica\" and \"Green Chemistry\".\n",
    "\n",
    "All 5 of these authors are asian and it is likely that they are so well connected in the network due to the fact that they are from a country with a large population and a large number of researchers, and therefore can connect easily peers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 nodes by degree:\n",
      "('https://openalex.org/A5055838753', 428) Chao Wang\n",
      "('https://openalex.org/A5090366405', 417) Hui Wang\n",
      "('https://openalex.org/A5022499603', 313) Hao Chen\n",
      "('https://openalex.org/A5015195367', 284) Qi Wang\n",
      "('https://openalex.org/A5002318539', 189) Xiao Zhang\n"
     ]
    }
   ],
   "source": [
    "## find top 5 nodes by number of degrees\n",
    "top5_nodes = sorted(G.degree, key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"Top 5 nodes by degree:\")\n",
    "for node in top5_nodes:\n",
    "    print(node, G.nodes[node[0]][\"display_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in largest connected component: 9329\n"
     ]
    }
   ],
   "source": [
    "## extract largests connected component\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "print(\"Number of nodes in largest connected component:\", len(largest_cc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "social",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
